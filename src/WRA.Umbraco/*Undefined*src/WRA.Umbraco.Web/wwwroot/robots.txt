# Allow all web crawlers to access all content initially, and then we filter them with rules below.
User-agent: *
Disallow:

# Sitemap for the site
Sitemap: https://www.wra.org/sitemap.xml

# Rate limit Googlebot to avoid server overload
User-agent: Googlebot
Crawl-delay: 10

# Prevent crawlers from accessing specific directories
Disallow: /umbraco/

# Block specific user agents that you don't want to crawl your site
User-agent: BadBot
Disallow: /

User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: Baiduspider
Disallow: /

User-agent: YandexBot
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: BLEXBot
Disallow: /

User-agent: Nessus
Disallow: /

User-agent: HeadlessChrome
Disallow: /

User-agent: curl
Disallow: /

# Allow access to specific files regardless of directory
# User-agent: *
# Allow: /umbraco/some-thing-bot-needs.html

# Disallow access to specific files regardless of directory
# Disallow: /some-file.txt